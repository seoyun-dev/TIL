{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9916196b-835f-4123-a895-ef5436c6d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 합성곱 기반 문장 분류 모델 정의\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # 학습된 임베딩 벡터\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "        )\n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "\n",
    "        conv = []\n",
    "        for size in filter_sizes:\n",
    "            conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels  = embedding_dim,\n",
    "                        out_channels = 1,\n",
    "                        kernel_size  = size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    ########################################원랜 -1 오타확인 (책 메모 참고)##################\n",
    "                    nn.MaxPool1d(kernel_size=max_length-size+1),\n",
    "                )\n",
    "            )\n",
    "        # Modulelist 클래스 : 여러 개의 서브 모듈을 리스트 형태로 저장\n",
    "        self.conv_filters = nn.ModuleList(conv)\n",
    "\n",
    "        output_size         = len(filter_sizes)\n",
    "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
    "        self.dropout        = nn.Dropout(dropout)\n",
    "        self.classifier     = nn.Linear(output_size, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # 원본 텐서의 차원 순서가 [0, 1, 2] (즉, [배치 크기, 시퀀스 길이, 임베딩 차원])에서 \n",
    "        # [0, 2, 1] (즉, [배치 크기, 임베딩 차원, 시퀀스 길이])로 변경되는 것을 의미 \n",
    "        # 이러한 재배열은 일반적으로 1D 합성곱 연산을 수행하기 전에 필요한 데이터 형태 조정을 위해 사용됨\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "\n",
    "        conv_outputs   = [conv(embeddings) for conv in self.conv_filters]\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
    "\n",
    "        logits = self.pre_classifier(concat_outputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.classifier(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec156fe-74b3-42cf-b1b1-f872da6edf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/seoyun/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/seoyun/Korpora/nsmc/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc71324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b108595c-5a3a-49e7-805c-3c26c8ccf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f07b10a-b0cc-468d-94a3-736c2e8dfcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ab46db-9134-48bd-afb7-56ed327a1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0cb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "embedding_dim = 128\n",
    "\n",
    "\n",
    "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in id_to_token.items():\n",
    "    if token not in [\"<pad>\", \"<unk>\"]:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1ea3c3-060f-4fd6-847f-d5cbc8fde989",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 합성곱 신경망 분류 모델 학습\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
    "classifier   = SentenceClassifier(\n",
    "    pretrained_embedding = init_embeddings,\n",
    "    filter_sizes         = filter_sizes,\n",
    "    max_length           = max_length\n",
    ").to(device) \n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4aa4b5-50fd-47e2-b332-f4e2821cbf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6801758408546448\n",
      "Train Loss 500 : 0.5805291992699553\n",
      "Train Loss 1000 : 0.5454219045577111\n",
      "Train Loss 1500 : 0.5311488998702651\n",
      "Train Loss 2000 : 0.5223988928403454\n",
      "Train Loss 2500 : 0.5159138447937609\n",
      "Val Loss : 0.46609184798150777, Val Accuracy : 0.7826\n",
      "Train Loss 0 : 0.43668437004089355\n",
      "Train Loss 500 : 0.48080227698037725\n",
      "Train Loss 1000 : 0.4797555399643672\n",
      "Train Loss 1500 : 0.4792746915212081\n",
      "Train Loss 2000 : 0.47937491081882155\n",
      "Train Loss 2500 : 0.47884384570408706\n",
      "Val Loss : 0.45873842495508466, Val Accuracy : 0.7822\n",
      "Train Loss 0 : 0.376381516456604\n",
      "Train Loss 500 : 0.46664320099020673\n",
      "Train Loss 1000 : 0.4715245616334778\n",
      "Train Loss 1500 : 0.47281620994478285\n",
      "Train Loss 2000 : 0.4689833408948602\n",
      "Train Loss 2500 : 0.47034597847877335\n",
      "Val Loss : 0.4486471884452497, Val Accuracy : 0.7872\n",
      "Train Loss 0 : 0.34489211440086365\n",
      "Train Loss 500 : 0.4596116668330933\n",
      "Train Loss 1000 : 0.4643681386967639\n",
      "Train Loss 1500 : 0.4633500731682317\n",
      "Train Loss 2000 : 0.4654974768171901\n",
      "Train Loss 2500 : 0.46564797392753254\n",
      "Val Loss : 0.45419559082665, Val Accuracy : 0.7856\n",
      "Train Loss 0 : 0.419948548078537\n",
      "Train Loss 500 : 0.4552552272638161\n",
      "Train Loss 1000 : 0.45609362226921124\n",
      "Train Loss 1500 : 0.45780373716616457\n",
      "Train Loss 2000 : 0.4595231677459991\n",
      "Train Loss 2500 : 0.46129254060762015\n",
      "Val Loss : 0.4436507445935624, Val Accuracy : 0.7896\n"
     ]
    }
   ],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval): \n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets): \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels    = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss   = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device): \n",
    "    model.eval()\n",
    "    losses   = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets): \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels    = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss   = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        # 분류 맞는지 확인\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs   = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64e324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
