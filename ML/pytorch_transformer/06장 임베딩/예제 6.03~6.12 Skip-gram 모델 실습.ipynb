{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a02c3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 기본 Skip-gram 클래스\n",
    "from torch import nn\n",
    "\n",
    "class VanilaSkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim  = embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features  = embedding_dim,\n",
    "            out_features = vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        output     = self.linear(embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f452e944-99e4-4584-b6a0-88ddc77cf953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/seoyun/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/seoyun/Korpora/nsmc/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "### 영화 리뷰 데이터세트 전처리\n",
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus = pd.DataFrame(corpus.test)  # 테스트세트 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f0eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
     ]
    }
   ],
   "source": [
    "# 데이터세트 형태소 추출\n",
    "tokenizer = Okt()\n",
    "tokens    = [tokenizer.morphs(review) for review in corpus.text]\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6242cddc-b96e-4cc3-8b14-6dd61a1e48b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "### 단어 사전 구축\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    # 특수토큰 - <unk>: OOV에 대응하기 위한 토큰, 단어 사전 내에 없는 모든 단어 대체\n",
    "    vocab = special_tokens\n",
    "    # 가장 많이 등장한 토큰 순서로 사전 구축\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "# 토큰화된 데이터로 단어 사전 구축 (n_vocab: 구축할 단어사전의 크기)\n",
    "vocab       = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))   # 최대 길이 5000 + 특수 토큰 1 = 5001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5be65c-a2c7-4a0a-b260-c69c5e27bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐'], ['야', '이'], ['야', '평점'], ['이', '뭐'], ['이', '야'], ['이', '평점']]\n"
     ]
    }
   ],
   "source": [
    "### Skip-gram의 단어 쌍 추출\n",
    "\n",
    "# 토큰을 입력받아 skip-gram의 학습 데이터로 사용하도록 전처리 - 1.(중심단어, 주변단어 쌍)\n",
    "def get_word_pairs(tokens, window_size):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        sentence_length = len(sentence)\n",
    "        for idx, center_word in enumerate(sentence):\n",
    "            window_start  = max(0, idx - window_size)\n",
    "            window_end    = min(sentence_length, idx + window_size + 1)\n",
    "            center_word   = sentence[idx]\n",
    "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
    "            for context_word in context_words:\n",
    "                pairs.append([center_word, context_word])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "word_pairs = get_word_pairs(tokens, window_size=2)\n",
    "print(word_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b3bb47-37b8-456a-8371-d1faaa4eda1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "### 인덱스 쌍 변환\n",
    "\n",
    "# 토큰을 입력받아 skip-gram의 학습 데이터로 사용하도록 전처리 - 2.인덱스 쌍으로 변환\n",
    "def get_index_pairs(word_pairs, token_to_id):\n",
    "    pairs = []\n",
    "    unk_index = token_to_id[\"<unk>\"]\n",
    "    for word_pair in word_pairs:\n",
    "        center_word, context_word = word_pair\n",
    "        # c.get(a, b): a가 c에 있으면 a의 인덱스, 없으면 b의 인덱스 반환\n",
    "        # 단어사전에 있으면 해당 토큰의 인덱스 반환, 없으면 <unk> 인덱스 반환\n",
    "        center_index  = token_to_id.get(center_word, unk_index)\n",
    "        context_index = token_to_id.get(context_word, unk_index)\n",
    "        pairs.append([center_index, context_index])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
    "print(index_pairs[:5])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4915d51c-6440-47e5-af3b-40c57923fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9y/9s104_6d69g1krb9mrzdrc6h0000gn/T/ipykernel_1647/554933739.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  index_pairs     = torch.tensor(index_pairs)\n"
     ]
    }
   ],
   "source": [
    "### 데이터로더 적용\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "index_pairs     = torch.tensor(index_pairs)\n",
    "center_indexs   = index_pairs[:, 0]\n",
    "contenxt_indexs = index_pairs[:, 1]\n",
    "\n",
    "# 토큰을 입력받아 skip-gram의 학습 데이터로 사용하도록 전처리 - 3.텐서로 변환\n",
    "dataset    = TensorDataset(center_indexs, contenxt_indexs)  # [N,2]구조\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa634889-1672-48d4-b0d8-a8758f03d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Skip-gram 모델 준비 작업\n",
    "from torch import optim\n",
    "\n",
    "device    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "word2vec  = VanilaSkipGram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "708ab10e-517c-497f-99c3-7ad5058f2022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1, Cost : 6.197\n",
      "Epoch :    2, Cost : 5.982\n",
      "Epoch :    3, Cost : 5.932\n",
      "Epoch :    4, Cost : 5.902\n",
      "Epoch :    5, Cost : 5.880\n",
      "Epoch :    6, Cost : 5.862\n",
      "Epoch :    7, Cost : 5.847\n",
      "Epoch :    8, Cost : 5.834\n",
      "Epoch :    9, Cost : 5.823\n",
      "Epoch :   10, Cost : 5.812\n"
     ]
    }
   ],
   "source": [
    "### 모델 학습\n",
    "\n",
    "for epoch in range(10):\n",
    "    cost = 0.0\n",
    "    for input_ids, target_ids in dataloader:\n",
    "        input_ids  = input_ids.to(device)   # center\n",
    "        target_ids = target_ids.to(device)  # context\n",
    "\n",
    "        logits = word2vec(input_ids)\n",
    "        loss   = criterion(logits, target_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "\n",
    "    cost = cost / len(dataloader)\n",
    "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82ee517a-ea9f-4feb-8eac-b57a28a3db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연기\n",
      "[ 8.3985800e-01  1.0761659e+00 -9.0439522e-01  4.3242589e-02\n",
      "  8.1296876e-02 -9.7479530e-02 -7.0416003e-01 -9.6324241e-01\n",
      " -1.4180402e+00  2.2226593e-01 -2.8250880e+00  1.8537556e-01\n",
      " -1.2204744e+00 -3.4485254e-02  7.6117694e-01 -4.1025135e-01\n",
      " -2.6833532e-02  2.9363859e-01 -8.7823009e-01  1.0064837e+00\n",
      " -8.0417120e-01  8.6418062e-01 -4.3136454e-01 -6.2688684e-01\n",
      " -1.0920253e+00  5.6999058e-01  5.2200222e-01 -1.0915482e+00\n",
      "  4.0858489e-01  1.5445745e+00  4.3164861e-01  8.7255138e-01\n",
      "  1.2707565e+00  6.8967521e-01 -3.2996199e-01  3.1246388e-02\n",
      " -1.7725719e+00  2.0509247e-01  1.8019468e-02 -6.6318232e-01\n",
      " -6.9887859e-01 -5.4264057e-01  5.0250506e-01  6.1741930e-01\n",
      " -2.2365169e-01  1.0105361e+00  8.0315655e-01  4.5288074e-01\n",
      "  1.4565660e-01 -6.0031480e-01 -3.8159466e-01  2.6690510e-01\n",
      "  2.1939619e+00 -1.1505514e+00  3.8696733e-01  2.8555635e-01\n",
      " -2.6111897e-02  7.7953339e-01  2.9616660e-01 -1.0148469e-01\n",
      " -1.1331215e+00 -1.1230891e+00  3.6436760e-01 -6.2794805e-01\n",
      " -1.8235543e-01 -7.2196352e-01 -1.5686080e+00 -6.2227163e-02\n",
      " -2.1852310e-01  3.7410736e-01 -7.0497379e-02 -6.9997007e-01\n",
      "  1.2405790e+00  1.5346584e-01  4.5678905e-01  6.8664539e-01\n",
      "  3.2030216e-01 -6.2819541e-01 -1.1812278e-01 -1.1151978e+00\n",
      " -1.0595089e+00  1.2554838e-01  2.4460554e+00 -2.7152523e-01\n",
      "  1.5389562e-03 -3.7708887e-01 -4.0939239e-01 -6.1880898e-01\n",
      "  9.4763184e-01  5.4947537e-01 -7.8451388e-02 -4.2697492e-01\n",
      "  1.3888337e-01  6.4936638e-01  1.3191423e+00 -1.9301690e+00\n",
      " -1.6240265e+00 -1.0398653e+00  1.5470983e+00 -1.2834797e+00\n",
      " -2.9972270e-01 -6.7009312e-01  5.1832569e-01  2.4913074e-02\n",
      " -9.5384216e-01  1.0441494e+00  2.9243058e-01  9.9231571e-01\n",
      " -1.5840007e-01  7.7417094e-01 -1.3533072e-01  6.4011782e-02\n",
      "  1.2619417e+00  1.1403943e+00 -9.4727445e-01  8.3840229e-02\n",
      " -7.2541058e-01  2.6142427e-01 -6.0228723e-01  2.2485375e+00\n",
      "  9.5656556e-01  1.3736163e-01 -4.5073044e-01  1.2393763e+00\n",
      " -2.9884920e-01  2.9998088e-01 -2.6507044e-01  9.8324364e-01]\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "### 임베딩 값 추출\n",
    "token_to_embedding = dict()\n",
    "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, embedding in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = embedding \n",
    "\n",
    "index = 30\n",
    "token = vocab[30]\n",
    "token_embedding = token_to_embedding[token]\n",
    "print(token)\n",
    "print(token_embedding)\n",
    "print(len(token_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94ea695e-8c70-4736-a427-b61322011344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "640128\n",
      "연기와 가장 유사한 5 개 단어\n",
      "및 - 유사도 : 0.2903\n",
      "되다니 - 유사도 : 0.2816\n",
      "롱 - 유사도 : 0.2735\n",
      "파이팅 - 유사도 : 0.2723\n",
      "만들었다 - 유사도 : 0.2634\n"
     ]
    }
   ],
   "source": [
    "### 단어 임베딩 유사도 계산\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# 입력 단어와 단언 사전 내의 모든 단어의 코사인 유사도 반환\n",
    "def cosine_similarity(a, b):\n",
    "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
    "    return cosine\n",
    "\n",
    "# 가장 가까운 단어 n개 반환\n",
    "def top_n_index(cosine_matrix, n):\n",
    "    # 내림차순하여 인덱스 반환\n",
    "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
    "    top_n = closest_indexes[1 : n + 1]\n",
    "    return top_n\n",
    "\n",
    "# token_embedding[128,1], embedding_matrix[5001,128]\n",
    "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
    "top_n = top_n_index(cosine_matrix, n=5)\n",
    "\n",
    "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
    "for index in top_n:\n",
    "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
