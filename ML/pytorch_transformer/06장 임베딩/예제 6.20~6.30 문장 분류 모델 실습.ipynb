{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcbdc2e",
   "metadata": {},
   "source": [
    "### 무작위로 초기화된 임베딩 벡터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8e1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 문장 분류 모델\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type=\"lstm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = n_vocab,\n",
    "            embedding_dim  = embedding_dim,\n",
    "            padding_idx    = 0\n",
    "        )\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size    = embedding_dim,\n",
    "                hidden_size   = hidden_dim,\n",
    "                num_layers    = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout       = dropout,\n",
    "                batch_first   = True,\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size    = embedding_dim,\n",
    "                hidden_size   = hidden_dim,\n",
    "                num_layers    = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout       = dropout,\n",
    "                batch_first   = True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings  = self.embedding(inputs)\n",
    "        output, _   = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]  # 마지막 시점의 결괏값만 분리\n",
    "        # 분류기 계층에 전달\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits      = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f61d2a-a53b-4e37-b118-927bb59d346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/seoyun/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/seoyun/Korpora/nsmc/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus    = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a11418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "### 데이터세트 불러오기\n",
    "\n",
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test  = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71519fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "### 데이터 토큰화 및 단어 사전 구축\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "tokenizer    = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens  = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a1c0ab-dd85-4f97-b013-c1e180141f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "### 정수 인코딩 및 패딩\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 최대길이(max_length)를 기준으로 잘라내거나 패딩\n",
    "def pad_sequences(sequences, max_length, pad_value): \n",
    "    result = list()\n",
    "    for sequence in sequences: \n",
    "        sequence        = sequence[:max_length]\n",
    "        pad_length      = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "unk_id    = token_to_id[\"<unk>\"]\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id     = token_to_id[\"<pad>\"]\n",
    "train_ids  = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids   = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866ad18f-31e7-4480-8460-d0b3e0c8ad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9y/9s104_6d69g1krb9mrzdrc6h0000gn/T/ipykernel_9414/1303580952.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "/var/folders/9y/9s104_6d69g1krb9mrzdrc6h0000gn/T/ipykernel_9414/1303580952.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_ids  = torch.tensor(test_ids)\n"
     ]
    }
   ],
   "source": [
    "### 데이터로더 적용\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids  = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels  = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset  = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3994b8de-7656-42b9-b191-9bceaddd9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 손실함수와 최적화함수 정의\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "n_vocab       = len(token_to_id)\n",
    "hidden_dim    = 64\n",
    "embedding_dim = 128\n",
    "n_layers      = 2\n",
    "\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab = n_vocab, hidden_dim = hidden_dim, embedding_dim = embedding_dim, n_layers = n_layers\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9227be-a546-4157-b472-c6d66d6897ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6906307935714722\n",
      "Train Loss 500 : 0.6938317327680226\n",
      "Train Loss 1000 : 0.6850458616679246\n",
      "Train Loss 1500 : 0.667911120766723\n",
      "Train Loss 2000 : 0.6565387439483527\n",
      "Train Loss 2500 : 0.6489575155040638\n",
      "Val Loss : 0.5854749915698847, Val Accuracy : 0.6994\n",
      "Train Loss 0 : 0.47263532876968384\n",
      "Train Loss 500 : 0.5422476583909608\n",
      "Train Loss 1000 : 0.5240420766703257\n",
      "Train Loss 1500 : 0.5100836372629632\n",
      "Train Loss 2000 : 0.4995229266796155\n",
      "Train Loss 2500 : 0.4893557622653396\n",
      "Val Loss : 0.43305313458648353, Val Accuracy : 0.7966\n",
      "Train Loss 0 : 0.30962520837783813\n",
      "Train Loss 500 : 0.39415990480197644\n",
      "Train Loss 1000 : 0.3939001073095051\n",
      "Train Loss 1500 : 0.3927157275765876\n",
      "Train Loss 2000 : 0.3938581429101687\n",
      "Train Loss 2500 : 0.3929202196637734\n",
      "Val Loss : 0.4027989142523787, Val Accuracy : 0.816\n",
      "Train Loss 0 : 0.2241000086069107\n",
      "Train Loss 500 : 0.33796470048660765\n",
      "Train Loss 1000 : 0.32974154361850255\n",
      "Train Loss 1500 : 0.33788463864711266\n",
      "Train Loss 2000 : 0.3386702847307262\n",
      "Train Loss 2500 : 0.3410364790383528\n",
      "Val Loss : 0.38898415906360734, Val Accuracy : 0.8182\n",
      "Train Loss 0 : 0.2035706341266632\n",
      "Train Loss 500 : 0.30208200566515236\n",
      "Train Loss 1000 : 0.2975828567674229\n",
      "Train Loss 1500 : 0.3019493038007254\n",
      "Train Loss 2000 : 0.30087341806259765\n",
      "Train Loss 2500 : 0.30378873230534187\n",
      "Val Loss : 0.40091156278745815, Val Accuracy : 0.8234\n"
     ]
    }
   ],
   "source": [
    "### 모델 학습 및 테스트\n",
    "\n",
    "def train(model, datasets, criterion, optimizer, device, interval): \n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets): \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels    = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss   = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device): \n",
    "    model.eval()\n",
    "    losses   = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets): \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels    = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss   = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs   = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee7ee60e-31ca-4722-85d1-00c1d48bc66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [ 4.6381968e-01  4.5853284e-01  1.1042061e+00 -2.8630337e-01\n",
      " -2.3868809e+00  1.8186281e+00  5.6016415e-02  1.7269763e+00\n",
      " -4.8994631e-01  2.3241314e-01 -6.2607592e-01  1.5056326e+00\n",
      "  1.4955376e+00 -9.4073707e-01 -2.2985396e+00  4.1764311e-02\n",
      " -5.3232628e-01 -5.6993902e-01 -9.5013803e-01 -1.7728597e+00\n",
      "  1.0996594e+00 -1.1447600e+00  1.3541558e-01 -5.0945348e-01\n",
      "  1.7277492e-02  9.5294577e-01  5.8853292e-01 -1.3104263e-01\n",
      " -7.9997852e-02  5.8746483e-02  8.9699215e-01  1.2525725e+00\n",
      " -3.8871145e-01  6.8464339e-01  1.2854266e+00  2.9320785e-01\n",
      "  8.0701023e-01  1.0742013e+00  5.0337810e-02  8.9767778e-01\n",
      " -1.4691917e+00  2.3550589e-01  4.7478580e-01 -1.1573229e+00\n",
      " -1.5816892e+00  2.2993772e+00  1.6551023e+00 -7.6844889e-01\n",
      "  9.7413224e-01  8.2299095e-01 -8.1355536e-01 -1.8070993e+00\n",
      " -2.8115697e-03 -4.9761187e-02  9.5386863e-01  2.1064138e+00\n",
      " -1.6204534e+00 -1.4959038e+00  2.4197862e-02 -9.8601902e-01\n",
      " -6.7798823e-02  7.7265567e-01  9.2326939e-01  1.3727677e+00\n",
      "  1.7368157e+00 -1.5792121e+00 -1.6664248e+00 -1.1027546e+00\n",
      "  1.0931791e+00  1.9795727e-02  6.3673502e-01  7.5129348e-01\n",
      "  6.8085867e-01 -4.5331209e-03  1.4076370e-01 -1.2877727e+00\n",
      " -2.1022585e-01  3.2854119e-01 -3.8298091e-01 -1.2495375e+00\n",
      " -1.3251646e+00 -1.2600399e+00  4.2868626e-01 -3.9098975e-01\n",
      " -9.9202526e-01 -1.2499362e-02 -1.8402128e+00 -1.5501599e-01\n",
      "  5.8048725e-01  1.6207102e-01  1.1717736e+00  1.4248120e-02\n",
      "  4.0323934e-01 -1.1972233e-01 -1.1565771e+00  2.5931567e-01\n",
      " -1.9243660e+00  2.4075234e-01  2.0146632e+00  2.3184250e-01\n",
      "  1.1172634e+00  9.6540558e-01  1.5455091e+00 -1.1024809e+00\n",
      "  3.5114551e-01 -4.2953253e-01 -3.5184667e-02  7.1269405e-01\n",
      " -1.5887109e+00  2.2967592e-01 -1.1359105e+00 -1.0028161e+00\n",
      " -2.5363207e-01  4.8739061e-01 -3.0823737e-01 -1.1759917e+00\n",
      "  7.5886309e-01 -1.2740338e-02 -1.8822491e+00 -8.9324027e-01\n",
      " -8.8316381e-01  1.9517159e-01  7.2890706e-04  1.6465141e+00\n",
      " -3.5690483e-01 -1.6242291e-01 -8.3557022e-01  3.6590168e-01]\n"
     ]
    }
   ],
   "source": [
    "### 학습된 모델로부터 임베딩 추출\n",
    "\n",
    "token_to_embedding = dict()\n",
    "embedding_matrix   = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix): \n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdaf2ed",
   "metadata": {},
   "source": [
    "### 사전 학습된 임베딩 값을 초깃값으로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "242b7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 사전 학습된 모델로 임베딩 계층 초기화\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word2vec        = Word2Vec.load(\"../models/word2vec.model\")\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in id_to_token.items():\n",
    "    if token not in [\"<pad>\", \"<unk>\"]:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "071f8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 사전학습된 임베딩 계층 적용\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type=\"lstm\",\n",
    "        ### -------추가-------\n",
    "        pretrained_embedding=None\n",
    "        ### -------추가-------\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = n_vocab,\n",
    "            embedding_dim  = embedding_dim,\n",
    "            padding_idx    = 0\n",
    "        )\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size    = embedding_dim,\n",
    "                hidden_size   = hidden_dim,\n",
    "                num_layers    = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout       = dropout,\n",
    "                batch_first   = True,\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size    = embedding_dim,\n",
    "                hidden_size   = hidden_dim,\n",
    "                num_layers    = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout       = dropout,\n",
    "                batch_first   = True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ### -------추가-------\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings = n_vocab,\n",
    "                embedding_dim  = embedding_dim,\n",
    "                padding_idx    = 0\n",
    "            )\n",
    "        ### -------추가-------\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings  = self.embedding(inputs)\n",
    "        output, _   = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]  # 마지막 시점의 결괏값만 분리\n",
    "        # 분류기 계층에 전달\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits      = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1390c905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6896252036094666\n",
      "Train Loss 500 : 0.6652128881442094\n",
      "Train Loss 1000 : 0.6565358474121227\n",
      "Train Loss 1500 : 0.6461410267523017\n",
      "Train Loss 2000 : 0.6407335017812902\n",
      "Train Loss 2500 : 0.6421096345297292\n",
      "Val Loss : 0.6435661743433712, Val Accuracy : 0.668\n",
      "Train Loss 0 : 0.7451693415641785\n",
      "Train Loss 500 : 0.602105374405246\n",
      "Train Loss 1000 : 0.6142179651277049\n",
      "Train Loss 1500 : 0.6076423599075111\n",
      "Train Loss 2000 : 0.5885867872263776\n",
      "Train Loss 2500 : 0.5769679590743907\n",
      "Val Loss : 0.5091475683469742, Val Accuracy : 0.7548\n",
      "Train Loss 0 : 0.5844336748123169\n",
      "Train Loss 500 : 0.5169892067799787\n",
      "Train Loss 1000 : 0.5144198542827374\n",
      "Train Loss 1500 : 0.5077311032954889\n",
      "Train Loss 2000 : 0.5002851151246419\n",
      "Train Loss 2500 : 0.4962731299341702\n",
      "Val Loss : 0.46782764916221936, Val Accuracy : 0.7738\n",
      "Train Loss 0 : 0.5001537799835205\n",
      "Train Loss 500 : 0.46687558525336714\n",
      "Train Loss 1000 : 0.4618528429325763\n",
      "Train Loss 1500 : 0.4606963502395796\n",
      "Train Loss 2000 : 0.4591790795370914\n",
      "Train Loss 2500 : 0.45671239825355103\n",
      "Val Loss : 0.4418162244101302, Val Accuracy : 0.7992\n",
      "Train Loss 0 : 0.5922149419784546\n",
      "Train Loss 500 : 0.4440325986363455\n",
      "Train Loss 1000 : 0.4439748217920204\n",
      "Train Loss 1500 : 0.4444471985935609\n",
      "Train Loss 2000 : 0.44078514981514094\n",
      "Train Loss 2500 : 0.43906630812192715\n",
      "Val Loss : 0.4240235367331642, Val Accuracy : 0.8028\n"
     ]
    }
   ],
   "source": [
    "### 사전 학습된 임베딩을 사용한 모델 학습\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, \n",
    "n_layers=n_layers, pretrained_embedding=init_embeddings\n",
    ").to(device)\n",
    "criterion  = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer  = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
    "\n",
    "epochs   = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
